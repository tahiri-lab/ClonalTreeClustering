# -*- coding: utf-8 -*-
"""
plot_dashboard.py
Multi-panel dashboard from a results_*.csv generated by grid_experiments.py.

Compatible with old/new CSV schemas:
- Old: timestamp, metric, k, L, n_per_group, noise, plevel, repeat, ARI, objective, seed
- New: timestamp, metric, k, L, n_per_group, noise, noise_mode, plevel, repeat, ARI, objective, coherence_measured, seed
- Mixed (same file with lines of both types) handled automatically.
Panels:
(a) ARI vs. noise (one curve per K)
(b) ARI vs. #leaves L (filtered to noise_ref)
(c) ARI vs. #trees/cluster n (filtered to noise_ref)
(d) ARI vs. coherence (plevel, or 'coherence_measured' if --use_measured_coherence)
(e) ARI vs. K (filtered to noise_ref)
(f) Î”ARI (noise_low - noise_high) per K
"""

import argparse
import csv
import math
from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt


# ------------------------- utilities -------------------------

def safe_float(x):
    if x is None:
        return float("nan")
    xs = str(x).strip()
    if xs == "" or xs.lower() == "nan":
        return float("nan")
    return float(xs)

def safe_int(x):
    return int(float(x))

def _parse_row_mixed(header, cols):
    """
   Returns a normalized dict with the following keys:
metric, k, L, n, noise, noise_mode, plevel, repeat, ARI, objective, coherence_measured, seed
covering old/new schema and mixed files.
"""
    # Clean
    cols = [c.strip() for c in cols]
    H = [h.strip() for h in header]
    out = {
        "metric": "", "k": 0, "L": 0, "n": 0,
        "noise": float("nan"), "noise_mode": "",
        "plevel": float("nan"), "repeat": 0,
        "ARI": float("nan"), "objective": float("nan"),
        "coherence_measured": float("nan"), "seed": ""
    }

    # If the header already includes noise_mode/coherence_measured, on map directly
    if ("noise_mode" in H) and ("coherence_measured" in H):
        d = dict(zip(H, cols))
        out["metric"] = d.get("metric","")
        out["k"]      = safe_int(d.get("k","0"))
        out["L"]      = safe_int(d.get("L","0"))
        out["n"]      = safe_int(d.get("n_per_group","0"))
        out["noise"]  = safe_float(d.get("noise"))
        out["noise_mode"] = d.get("noise_mode","").strip()
        out["plevel"] = safe_float(d.get("plevel"))
        out["repeat"] = safe_int(d.get("repeat","0"))
        out["ARI"]    = safe_float(d.get("ARI"))
        out["objective"] = safe_float(d.get("objective"))
        out["coherence_measured"] = safe_float(d.get("coherence_measured"))
        out["seed"]   = d.get("seed","")
        return out

    # Otherwise, the header is old. We look at the length of the line.
    ncols = len(cols)
    # Pure old diagram (11 columns)
    if ncols == 11:
        # idx: 0 ts,1 metric,2 k,3 L,4 n,5 noise,6 plevel,7 repeat,8 ARI,9 objective,10 seed
        out["metric"] = cols[1]
        out["k"]      = safe_int(cols[2])
        out["L"]      = safe_int(cols[3])
        out["n"]      = safe_int(cols[4])
        out["noise"]  = safe_float(cols[5])
        out["plevel"] = safe_float(cols[6])
        out["repeat"] = safe_int(cols[7])
        out["ARI"]    = safe_float(cols[8])
        out["objective"] = safe_float(cols[9])
        out["seed"]   = cols[10]
        return out

    # New complete diagram (13 columns) but with old header at the head of the file
    if ncols >= 13:
        # idx (nouveau): 0 ts,1 metric,2 k,3 L,4 n,5 noise,6 noise_mode,7 plevel,8 repeat,9 ARI,10 objective,11 coh,12 seed
        out["metric"] = cols[1]
        out["k"]      = safe_int(cols[2])
        out["L"]      = safe_int(cols[3])
        out["n"]      = safe_int(cols[4])
        out["noise"]  = safe_float(cols[5])
        out["noise_mode"] = cols[6]
        out["plevel"] = safe_float(cols[7])
        out["repeat"] = safe_int(cols[8])
        out["ARI"]    = safe_float(cols[9])
        out["objective"] = safe_float(cols[10])
        out["coherence_measured"] = safe_float(cols[11])
        out["seed"]   = cols[12]
        return out

    # Case 12 columns (partial mix). Heuristic detection:
    # Either noise_mode present but not coherence_measured, or coherence without noise_mode.
    if ncols == 12:
        # Check if col[6] looks like noise_mode
        if cols[6].lower() in ("swap","drop"):
            # 0 ts,1 metric,2 k,3 L,4 n,5 noise,6 noise_mode,7 plevel,8 repeat,9 ARI,10 objective,11 seed
            out["metric"] = cols[1]
            out["k"]      = safe_int(cols[2])
            out["L"]      = safe_int(cols[3])
            out["n"]      = safe_int(cols[4])
            out["noise"]  = safe_float(cols[5])
            out["noise_mode"] = cols[6]
            out["plevel"] = safe_float(cols[7])
            out["repeat"] = safe_int(cols[8])
            out["ARI"]    = safe_float(cols[9])
            out["objective"] = safe_float(cols[10])
            out["seed"]   = cols[11]
            return out
        else:
            # Probable old diagram + a column of coherence at the end
            # 0 ts,1 metric,2 k,3 L,4 n,5 noise,6 plevel,7 repeat,8 ARI,9 objective,10 seed,11 coh
            out["metric"] = cols[1]
            out["k"]      = safe_int(cols[2])
            out["L"]      = safe_int(cols[3])
            out["n"]      = safe_int(cols[4])
            out["noise"]  = safe_float(cols[5])
            out["plevel"] = safe_float(cols[6])
            out["repeat"] = safe_int(cols[7])
            out["ARI"]    = safe_float(cols[8])
            out["objective"] = safe_float(cols[9])
            out["seed"]   = cols[10]
            out["coherence_measured"] = safe_float(cols[11])
            return out

    # Fallback: try a simple zip (can ignore extra columns)
    d = dict(zip(H, cols))
    out["metric"] = d.get("metric","")
    out["k"]      = safe_int(d.get("k","0"))
    out["L"]      = safe_int(d.get("L","0"))
    out["n"]      = safe_int(d.get("n_per_group","0"))
    out["noise"]  = safe_float(d.get("noise"))
    out["noise_mode"] = d.get("noise_mode","")
    out["plevel"] = safe_float(d.get("plevel"))
    out["repeat"] = safe_int(d.get("repeat","0"))
    out["ARI"]    = safe_float(d.get("ARI"))
    out["objective"] = safe_float(d.get("objective"))
    out["coherence_measured"] = safe_float(d.get("coherence_measured"))
    out["seed"]   = d.get("seed","")
    return out

def load_results(path):
    rows = []
    with open(path, newline="", encoding="utf-8") as f:
        reader = csv.reader(f)
        header = next(reader)
        for cols in reader:
            if not cols:
                continue
            r = _parse_row_mixed(header, cols)
            rows.append(r)
    if not rows:
        raise RuntimeError(f"No rows parsed from {path}")
    return rows

def filter_rows(rows, metric_filter=None, plevel_filter=None, noise_mode_filter=None):
    out = []
    for r in rows:
        if metric_filter and r["metric"] != metric_filter:
            continue
        if plevel_filter is not None and (not math.isfinite(r["plevel"]) or abs(r["plevel"] - plevel_filter) > 1e-12):
            continue
        if noise_mode_filter and r["noise_mode"] != noise_mode_filter:
            continue
        out.append(r)
    return out

def agg_mean(rows, keys, value_key="ARI"):
    acc = defaultdict(list)
    for r in rows:
        v = r.get(value_key, float("nan"))
        if v == v:  # non-NaN
            acc[tuple(r[k] for k in keys)].append(v)
    return {k: float(np.mean(vs)) for k, vs in acc.items() if vs}

def sort_unique(vals):
    return sorted(set(vals))

def choose_marker(i):
    markers = ["o", "s", "D", "^", "v", ">", "<", "P", "X", "*"]
    return markers[i % len(markers)]


# ------------------------- plotting -------------------------

def main():
    ap = argparse.ArgumentParser(description="Plot multi-panel dashboard from results CSV.")
    ap.add_argument("--results", required=True, help="results_wmfd.csv")
    ap.add_argument("--out", required=True, help="output PNG path")
    ap.add_argument("--noise_ref", type=float, default=0.50)
    ap.add_argument("--noise_low", type=float, default=0.10)
    ap.add_argument("--noise_high", type=float, default=0.75)
    ap.add_argument("--metric_filter", default=None, help="e.g. wmfd or rf")
    ap.add_argument("--plevel_filter", type=float, default=None, help="Fix plevel to make curves comparable")
    ap.add_argument("--use_measured_coherence", action="store_true",
                    help="Use 'coherence_measured' for panel (d) instead of 'plevel'")
    ap.add_argument("--noise_mode_filter", default=None, choices=[None, "swap", "drop"],
                    help="Optional filter to only plot a given noise mode")
    args = ap.parse_args()

    rows = load_results(args.results)
    rows = filter_rows(rows,
                       metric_filter=args.metric_filter,
                       plevel_filter=args.plevel_filter,
                       noise_mode_filter=args.noise_mode_filter)

    if not rows:
        raise RuntimeError("No rows left after filtering. Check your filters and CSV.")

    fig = plt.figure(figsize=(14, 10), dpi=140)
    gs = fig.add_gridspec(2, 3, wspace=0.25, hspace=0.35)

    # (a) ARI vs noise
    ax_a = fig.add_subplot(gs[0, 0])
    ax_a.set_title("(a) ARI vs noise")
    ax_a.set_xlabel("noise (%)")
    ax_a.set_ylabel("ARI")
    m_a = agg_mean(rows, keys=["k", "noise"], value_key="ARI")
    Ks = sort_unique([k for (k, _n) in m_a.keys()])
    for i, k in enumerate(Ks):
        xs, ys = [], []
        for (kk, nval), mean_ari in sorted(m_a.items()):
            if kk != k:
                continue
            xs.append(int(round(nval * 100)))
            ys.append(mean_ari)
        if xs:
            ax_a.plot(xs, ys, marker=choose_marker(i), label=f"K={k}")
    ax_a.set_ylim(0, 1)
    ax_a.grid(True, alpha=0.3)
    if Ks:
        ax_a.legend(frameon=False, fontsize=8)

    # (b) ARI vs L (noise_ref)
    ax_b = fig.add_subplot(gs[0, 1])
    ax_b.set_title(f"(b) ARI vs #leaves (noise={int(args.noise_ref*100)}%)")
    ax_b.set_xlabel("L (leaves)")
    ax_b.set_ylabel("ARI")
    rows_b = [r for r in rows if abs(r["noise"] - args.noise_ref) < 1e-12]
    m_b = agg_mean(rows_b, keys=["L"], value_key="ARI")
    xs = sorted(m_b.keys())
    if xs:
        x = [x_[0] for x_ in xs]
        y = [m_b[t] for t in xs]
        ax_b.plot(x, y, marker="o")
    ax_b.set_ylim(0, 1)
    ax_b.grid(True, alpha=0.3)

    # (c) ARI vs n (noise_ref)
    ax_c = fig.add_subplot(gs[0, 2])
    ax_c.set_title(f"(c) ARI vs #trees/cluster (noise={int(args.noise_ref*100)}%)")
    ax_c.set_xlabel("n per cluster")
    ax_c.set_ylabel("ARI")
    m_c = agg_mean(rows_b, keys=["n"], value_key="ARI")
    xs = sorted(m_c.keys())
    if xs:
        x = [x_[0] for x_ in xs]
        y = [m_c[t] for t in xs]
        ax_c.plot(x, y, marker="o")
    ax_c.set_ylim(0, 1)
    ax_c.grid(True, alpha=0.3)

    # (d) ARI vs coherence
    ax_d = fig.add_subplot(gs[1, 0])
    if args.use_measured_coherence and any(math.isfinite(r.get("coherence_measured", float("nan"))) for r in rows):
        ax_d.set_title("(d) ARI vs intra-cluster coherence (leaf Jaccard)")
        ax_d.set_xlabel("coherence (%)")
        buckets = defaultdict(list)
        for r in rows:
            v = r.get("coherence_measured", float("nan"))
            if v == v:
                buckets[round(v, 3)].append(r["ARI"])
        xs = sorted(buckets.keys())
        x = [int(round(100 * v)) for v in xs]
        y = [float(np.mean(buckets[v])) for v in xs]
        if x:
            ax_d.plot(x, y, marker="o")
    else:
        ax_d.set_title("(d) ARI vs plevel (generator coherence)")
        ax_d.set_xlabel("plevel (%)")
        m_d = agg_mean(rows, keys=["plevel"], value_key="ARI")
        xs = sorted(m_d.keys())
        if xs:
            x = [int(round(100 * t[0])) for t in xs]
            y = [m_d[t] for t in xs]
            ax_d.plot(x, y, marker="o")
    ax_d.set_ylabel("ARI")
    ax_d.set_ylim(0, 1)
    ax_d.grid(True, alpha=0.3)

    # (e) ARI vs K (noise_ref)
    ax_e = fig.add_subplot(gs[1, 1])
    ax_e.set_title(f"(e) ARI vs K (noise={int(args.noise_ref*100)}%)")
    ax_e.set_xlabel("K")
    ax_e.set_ylabel("ARI")
    m_e = agg_mean(rows_b, keys=["k"], value_key="ARI")
    xs = sorted(m_e.keys())
    if xs:
        x = [x_[0] for x_ in xs]
        y = [m_e[t] for t in xs]
        ax_e.plot(x, y, marker="o")
    ax_e.set_ylim(0, 1)
    ax_e.grid(True, alpha=0.3)

    # (f) Î”ARI (low - high)  K
    ax_f = fig.add_subplot(gs[1, 2])
    ax_f.set_title(f"(f) Î”ARI (noise {int(args.noise_low*100)}% - {int(args.noise_high*100)}%) by K")
    ax_f.set_xlabel("K")
    ax_f.set_ylabel("Î”ARI")
    rows_low = [r for r in rows if abs(r["noise"] - args.noise_low) < 1e-12]
    rows_high = [r for r in rows if abs(r["noise"] - args.noise_high) < 1e-12]
    m_low = agg_mean(rows_low, keys=["k"], value_key="ARI")
    m_high = agg_mean(rows_high, keys=["k"], value_key="ARI")
    Ks_all = sorted(set([k for (k,) in m_low.keys()] + [k for (k,) in m_high.keys()]))
    if Ks_all:
        x = Ks_all
        y = []
        for k in Ks_all:
            a = m_low.get((k,), float("nan"))
            b = m_high.get((k,), float("nan"))
            y.append((a - b) if (a == a and b == b) else float("nan"))
        ax_f.plot(x, y, marker="o")
    ax_f.grid(True, alpha=0.3)

    fig.suptitle("Clustering performance dashboard", fontsize=14)
    plt.savefig(args.out, bbox_inches="tight")
    print(f"[OK] Wrote {args.out}")


if __name__ == "__main__":
    main()
